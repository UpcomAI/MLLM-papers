# MLLM-papers
[![Awesome](https://awesome.re/badge.svg)](https://github.com/UpcomAI/MLLM-papers/) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/UpcomAI/MLLM-papers/blob/main/LICENSE)
![](https://img.shields.io/github/last-commit/UpcomAI/MLLM-papers?color=green) 
![](https://img.shields.io/badge/PRs-Welcome-red) 

> How to extend LLM-powered AI agents into the multimodal domain?

ðŸ™Œ This repository collects papers integrating **Multimodal Large Language Models (LLMs)**.

ðŸ˜Ž Welcome to recommend missing papers through **`Adding Issues`** or **`Pull Requests`**. 

ðŸ¥½ The papers are ranked according to our **subjective** opinions.

## ðŸ“œ Table of Content

- [MLLM-papers](#mllm-papers)
  - [ðŸ“œ Table of Content](#-table-of-content)
  - [âœ¨ï¸Ž Outstanding Papers](#ï¸Ž-outstanding-papers)
  - [ðŸ“¥ Paper Inbox](#-paper-inbox)
    - [Survey](#survey)
    - [Datasets \& Simulator](#datasets--simulator)
    - [Algorithms](#algorithms)
    - [Applications](#applications)
      - [Perception](#perception)
      - [Policy](#policy)
      - [Action](#action)
      - [Control](#control)
      - [Long Video Understanding](#long-video-understanding)
      - [LLM-based Video Agents](#llm-based-video-agents)
    - [System Implementation](#system-implementation)

## âœ¨ï¸Ž Outstanding Papers

- \[[arXiv 2024](https://arxiv.org/pdf/2402.15116)] Large Multimodal Agents: A Survey
- \[[arXiv 2024](https://arxiv.org/abs/2401.06805)\] Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning

## ðŸ“¥ Paper Inbox

### Survey

- \[[arXiv 2024](https://arxiv.org/pdf/2402.15116)] Large Multimodal Agents: A Survey
- \[[arXiv 2024](https://arxiv.org/abs/2401.06805)\] Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning

### Datasets \& Simulator

### Algorithms

### Applications

#### Long Video Understanding

- \[[arXiv 2024](https://arxiv.org/pdf/2404.04346)\] Koala: Key frame-conditioned long video-LLM
- \[[arXiv 2024](https://arxiv.org/pdf/2404.00308)\] ST-LLM: Large Language Models Are Effective Temporal Learners
- \[[arXiv 2024](https://arxiv.org/pdf/2404.05726)\] MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding
- \[[arXiv 2024](https://arxiv.org/pdf/2404.03384)\] LongVLM: Efficient Long Video Understanding via Large Language Models
- \[[arXiv 2024](https://arxiv.org/pdf/2403.11289)\] ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models
- \[[arXiv 2024](https://arxiv.org/pdf/2406.04325)\] ShareGPT4Video: Improving Video Understanding and Generation with Better Captions
- \[[arXiv 2024](https://arxiv.org/pdf/2405.17247)\] An Introduction to Vision-Language Modeling
- \[[arXiv 2023](https://arxiv.org/pdf/2307.16449v2)\] MovieChat: From Dense Token to Sparse Memory for Long Video Understanding
- \[[arXiv 2023](https://arxiv.org/pdf/2305.15021)\] EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought
- \[[arXiv 2023](https://arxiv.org/pdf/2306.02858)\] Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding

#### LLM-based Video Agents

- \[[arXiv 2023](https://arxiv.org/pdf/2304.04227)\] Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions
- \[[arXiv 2023](https://arxiv.org/pdf/2303.06594)\] ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions
- \[[arXiv 2023](https://arxiv.org/pdf/2311.18799)\] X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning
- \[[arXiv 2023](https://arxiv.org/pdf/2305.06500)\] InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning
- \[[arXiv 2023](https://arxiv.org/pdf/2304.14407)\] ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System
- \[[arXiv 2023](https://arxiv.org/pdf/2310.19773)\] MM-VID: Advancing Video Understanding with GPT-4V(ision)
- \[[arXiv 2023](https://arxiv.org/pdf/2311.17435)\] MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning
- \[[arXiv 2023](https://arxiv.org/pdf/2310.11699)\] MISAR: A Multimodal Instructional System with Augmented Reality
- \[[arXiv 2022](https://arxiv.org/pdf/2201.12086)\] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
- \[[arXiv 2022](https://arxiv.org/pdf/2204.00598)\] Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language

### System Implementation

- \[[SIGCHI 2024](https://dl.acm.org/doi/proceedings/10.1145/3610978?tocHeading=heading6)] Language, Camera, Autonomy! Prompt-engineered Robot Control for Rapidly Evolving Deployment (**CLEAR**) `[Software]`
- \[[Autonomous Robots 2023](https://link.springer.com/article/10.1007/s10514-023-10139-z)\] TidyBot: personalized robot assistance with large language models \[[Project](https://tidybot.cs.princeton.edu)\]
- \[[RSS 2024](https://roboticsconference.org/program/papers/016/)\]Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware (**ALOHA**)
- \[[arXiv 2024](https://arxiv.org/pdf/2404.03570)\] Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity `Dual-Arm`
  - \[[arXiv 2023](https://arxiv.org/pdf/2305.10403)\] Palm 2 technical report
  - \[[arXiv 2022](https://arxiv.org/pdf/2205.06230)\] Simple Open-Vocabulary Object Detection with Vision Transformers
  - \[[ICRA 2023](https://ieeexplore.ieee.org/abstract/document/10161283/)\] Robotic Table Wiping via Reinforcement Learning and Whole-body Trajectory Optimization
- \[[Paper 2024](https://humanoid-ai.github.io/HumanPlus.pdf)\] HumanPlus: Humanoid Shadowing and Imitation from Humans \[[Project](https://humanoid-ai.github.io)\] 
